\section{Existing OIE Systems}
    \todo{Add lit.sur. of oie systems from imojie/mlil paper}

    The existing OpenIE systems can be broadly categorised into two categories - Rule-Based Systems, and Neural Systems.

    \subsection{Rule-Based Systems}
        Traditional open extractors are rule-based or statistical, e.g., Textrunner \cite{banko&al07}, ReVerb \cite{Fader&al11,etzioni-ijcai11}, OLLIE \cite{Mausam&al12}, Stanford-IE \cite{angeli&al15}, ClausIE \cite{corro&al13}, OpenIE-4 \cite{christensen&al11,pal&al16}, OpenIE-5 \cite{Saha&al2017, Saha2018OpenIE}, PropS \cite{Stanovsky&al2016}, and MinIE \cite{gashteovski&al17}. These systems are largely unsupervised in nature, or bootstrapped from extractions made by earlier systems. They use syntactic or semantic parsers combined with rules to extract tuples from sentences.

        \todo{see if you can add some info for specific rule based systems}
        \subsubsection{ClausIE}
        \subsubsection{PropS}
        \subsubsection{OllIE}
        \subsubsection{OpenIE - 4}
        \subsubsection{OpenIE - 5}

    \subsection{Neural Systems}
        To bypass error accumulation in rule-based systems with multiple subcomponents, end-to-end neural systems have been proposed recently. They belong to one of two paradigms: sequence \textit{labeling} or sequence \textit{generation}.

        \subsubsection{Sequence Labelling}
            \emph{Sequence Labeling} involves tagging each word in the input sentence as belonging to the subject, predicate, object or other. The final extraction is obtained by collecting labeled spans into different fields and constructing a tuple. RnnOIE \citep{stanovsky&al18} is a labeling system that first identifies the relation words and then uses sequence labelling to get their arguments. It is trained on OIE2016 dataset, which postprocesses SRL data for OpenIE \citep{Stanovsky2016EMNLP}.

            SenseOIE \citep{roy&al19}, improves upon RnnOIE by using the extractions of multiple OpenIE systems as features in a sequence labeling setting. However, their training requires manually annotated gold extractions, which is not scalable for the task. This restricts SenseOIE to train on a dataset of 3,000 sentences. In contrast, our proposed \textit{Score-and-Filter} mechanism is unsupervised and can scale unboundedly. \citet{jiang12019improving} is another labeling system that better calibrates extractions across sentences.
    
            SpanOIE \citep{zhan&al19} uses a span selection model, a variant of the sequence labelling paradigm. Firstly, the predicate module finds the predicate spans in a sentence. Subsequently, the argument module outputs the arguments for this predicate. However, SpanOIE cannot extract nominal relations. Moreover, it bootstraps its training data over a single OpenIE system only. In contrast, \shortname{} overcomes both of these limitations. \todo{shift imojie mentions to imojie section}

        \subsubsection{Sequence Generation}
            \emph{Sequence Generation} uses a Seq2Seq model to generate output extractions one word at a time. The generated sequence contains field demarcators, which are used to convert the generated flat sequence to a tuple.
            
            Copy Attention \citep{cui&al18} is a neural generator trained over bootstrapped data generated from OpenIE-4 extractions on a large corpus. During inference, it uses beam search to get the predicted extractions. It uses a fixed-size beam, limiting it to output a constant number of extractions per sentence. Moreover, our analysis shows that CopyAttention extractions severely lack in diversity, as illustrated in Table~\ref{tab:redundancy-example}.
            
            Our analysis of Copy Attention reveals that it suffers from two drawbacks. First, it does not naturally adapt the number of extractions to the length or complexity of the input sentence.  Second, it is susceptible to \emph{stuttering}: extraction of multiple triples bearing redundant information.
            
            These limitations arise because its decoder has no explicit mechanism to remember what parts of the sentence have already been `consumed' or what triples have already been generated. Its decoder uses a fixed-size beam for inference. However, beam search can only ensure that the extractions are not exact duplicates.
        
            \citet{sun2018logician} propose the \emph{Logician} model, a restricted sequence generation model for extracting tuples from Chinese text. Logician relies on coverage attention and gated-dependency attention, a language-specific heuristic for Chinese. Using coverage attention, the model also tackles generation of multiple extractions while being globally-aware. We compare against Logician's coverage attention as one of the approaches for increasing diversity.

        \subsubsection{Comparison}
        Sequence-labeling based models lack the ability to change the sentence structure or introduce new auxiliary words while uttering predictions. For example, they cannot extract \verb|(Trump, is the PresidentÂ of, US)| from \verb|``US President Trump''|, since `is', `of' are not in the original sentence. Also, they assume that all words in one field of the extraction would occur in the same order as they are in the original sentence. But this may not always be ideal. 
    
        On the other hand, sequence-generation models subsume the former type of models. It comes with the added ability to generate words that are not present in the sentence as well as the ability to mutate the sentence structure.

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benckmarks}
    To the best of our knowledge, there were three benchmarks systems available for comparing Open IE systems - OIE2016, RelVis and Wire57.

    \subsection{OIE2016}
        The first and the most prominent is OIE2016 \cite{OIE2016}. This has been widely adopted as the standard evaluation framework to test new systems on (e.g., OIE2016 is used by the NST \citep{Nst} and Neural Open IE \citep{cui&al18} systems).  In OIE2016, gold tuples are generated using an automated rule-based system built on top of a QA-SRL dataset \cite{QA-SRL}. In early analysis we find this dataset to be rather noisy. Table \ref{tab:gold_example} illustrates some sample sentences from this gold dateset. These tuples look obviously wrong, and unfit to be in the gold set.

        In addition to the dataset, \citet{OIE2016} release a scorer that compares a set of gold tuples with a set of system tuples to estimate word-level precision and recall. This scorer has been identified to not penalize long extractions. It also does not penalise extractions for misidentifying parts of a relation in an argument slot (or vice versa), leading to trivial systems that score much better than genuine Open IE systems  \cite{Wire57}. We also observe that the scorer compares words all-to-all allowing multiple same words in an extraction to match a corresponding one in the gold. Thus, simply repeating a word in the extraction will give it a high precision score. Finally, the scorer loops over gold tuples in an arbitrary order, and matches them to predicted extractions in a sequential manner. Once a gold matches to a predicted extraction, it is rendered unavailable for any subsequent, potentially better-matched, extraction.

    \subsection{RelVis}
        Another dataset is RelVis \cite{Relvis}, a benchmark that borrows its data from four different datasets including OIE2016. Since OIE2016 forms a major part of this dataset, it has similar issues with noise. Its scorer makes some modifications to OIE2016. However, it does not reward partial coverage of gold tuples, and forces one system prediction to match just one gold. It also does not penalize overlong extractions.
        
    \subsection{Wire57}
        Finally, Wire57 \cite{Wire57} makes further improvements in the scorer. It  penalises overlong extractions and assigns a token-level precision and recall score to all gold-prediction pairs for a sentence. Moreover, it considers all pairs of extractions in its matching phase. However, it still forces one prediction to match just one gold. It also reports just one score for a system, ignoring the confidence values of the individual predictions that make the precision-recall curve of OIE2016 possible. Our scorer is inspired by theirs, with some changes. More importantly, the dataset used in Wire57 is manually curated, but with only 57 sentences, which is too small to suffice as a comprehensive test dataset.
        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Uncategorised
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

