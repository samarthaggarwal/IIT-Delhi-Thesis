
\section{Overview}

    We design the first neural OpenIE system that uses sequential decoding of tuples conditioned on previous tuples. We achieve this by adding every generated extraction so far to the encoder. This iterative process stops when the \textit{EndOfExtractions} tag is generated by the decoder, allowing it to produce a variable number of extractions. We name our system \boldlongname\ (\boldshortname).

    CopyAttention uses a bootstrapping strategy, where the extractions from  OpenIE-4 \cite{christensen&al11,pal&al16} are used as training data. However, we believe that training on extractions of multiple systems is preferable. For example, OpenIE-4 benefits from high precision compared to ClausIE \cite{corro&al13}, which offers high recall. By aggregating extractions from both, \shortname{} could potentially obtain a better precision-recall balance.

    However, simply concatenating extractions from multiple systems does not work well, as it leads to redundancy as well as exaggerated noise in the dataset. 
    We devise an unsupervised \textbf{Score-and-Filter} mechanism to automatically select a subset of these extractions that are non-redundant and expected to be of high quality. Our approach scores all extractions with a scoring model, followed by filtering to reduce redundancy.

    We compare \shortname\ against several neural and non-neural systems, including our extension of CopyAttention that uses BERT \cite{devlin&al19} instead of an LSTM at encoding time, which forms a very strong baseline. On the recently proposed CaRB metric, which penalizes redundant extractions \cite{bhardwaj&al19}, \shortname\ outperforms CopyAttention by about 18 pts in F1 and our strong BERT baseline by 2 pts, establishing a new state of the art for OpenIE. We release \shortname\ \& all related resources for further research\footnote{\protect\url{https://github.com/dair-iitd/imojie}}.

\section{Sequencial Decoding Model}

    % Seq decoding model
    \begin{figure*}[h]
    \includegraphics[width=.99\hsize]{images/imojie/model.png}
    \caption{One step of the sequential decoding process, for generating the $i^\text{th}$ extraction, which takes the original sentence and all extractions numbered $1,\ldots,i-1$, previously generated, as input.}
    \label{fig:seq_dec}
    \end{figure*}

    We now describe \shortname{}, our generative approach that can output a variable number of diverse extractions per sentence. The architecture of our model is illustrated in Figure \ref{fig:seq_dec}. At a high level, the next extraction from a sentence is best determined in context of all other tuples extracted from it so far.  Hence, \shortname{} uses a decoding strategy that generates extractions in a sequential fashion, one after another, each one being aware of all the ones generated prior to it.

    This kind of sequential decoding is made possible by the use of an \textit{iterative memory}. Each of the generated extractions are added to the memory so that the next iteration of decoding has access to all of the previous extractions. We simulate this iterative memory with the help of BERT encoder, whose input includes the [\textit{CLS}] token and original sentence appended with the decoded extractions so far, punctuated by the separator token [\textit{SEP}] before each extraction.

    \shortname{} uses an LSTM  decoder, which is initialized with the embedding of [\textit{CLS}] token. The contextualized-embeddings of all the word tokens are used for the Copy \citep{gu&al16} and Attention \citep{bahdanau&al15} modules. The decoder generates the tuple one word at a time, producing $\langle \textit{rel} \rangle$ and $\langle \textit{obj} \rangle$ tokens to indicate the start of relation and object respectively. The iterative process continues until the \textit{EndOfExtractions} token is generated.

    The overall process can be summarized as:
    \begin{enumerate}
        \item Pass the sentence through the Seq2Seq architecture to generate the first extraction.
        \item Concatenate the generated extraction with the existing input and pass it again through the Seq2Seq architecture to generate the next extraction.
        \item Repeat Step 2 until the \textit{EndOfExtractions} token is generated.
    \end{enumerate}

    \shortname{} is trained using a cross-entropy loss between the generated output and the gold output.


\section{Aggregating Bootstrapped Data}
    
    \subsection{Single Bootstrapping System}

        To train generative neural models for the task of OpenIE, we need a set of sentence-extraction pairs. It is ideal to curate such a training dataset via human annotation, but that is impractical, considering the scale of training data required for a neural model. We follow \citet{cui&al18}, and use  bootstrapping --- using extractions from a pre-existing OpenIE system as `silver'-labeled (as distinct from `gold'-labeled) instances to train the neural model. We first order all extractions in the decreasing order of confidences output by the original system. We then construct training data in \shortname's input-output format, assuming that this is the order in which it should produce its extractions.

    \subsection{Multiple Bootstrapping Systems}

        % score-and-filter
        \begin{figure*}[h]
            \includegraphics[width=.99\hsize]{images/imojie/score_filter.png}
            \caption{Ranking-Filtering subsystem for combining extractions from multiple open IE systems in an unsupervised fashion.  (`Exts'=extractions.)}
            \label{fig:score_filter}
        \end{figure*}

        Different OpenIE systems have diverse quality characteristics. For example, the human-estimated (precision, recall) of OpenIE-4 is $(\textbf{61},43)$ while that of ClausIE is $(40,\textbf{50})$. Thus, by using their combined extractions as the bootstrapping dataset, we might potentially benefit from the high precision of OpenIE-4 and high recall of ClausIE.

        However, simply pooling all extractions would not work, because of the following serious hurdles.
        \begin{description}
        \item[No calibration:] Confidence scores assigned by different systems are not calibrated to a comparable scale.
        \item[Redundant extractions:] Beyond exact duplicates, multiple systems produce similar extractions with low marginal utility.
        \item[Wrong extractions:] Pooling inevitably pollutes the silver data and can amplify incorrect instances, forcing the downstream open IE system to learn poor-quality extractions.
        \end{description}
        We solve these problems using a \textbf{Score-and-Filter} framework, shown in Figure~\ref{fig:score_filter}.

        \vspace{0.5ex}
        \noindent \textbf{Scoring:}
        All systems are applied on a given sentence, and the pooled set of extractions are scored such that good (correct, informative) extractions generally achieve higher values compared to bad (incorrect) and redundant ones.  In principle, this score may be estimated by the generation score from \shortname, trained on a single system. 
        
        In practice, such a system is likely to consider extractions similar to its bootstrapping training data as good, while disregarding extractions of other systems, even though those extractions may also be of high quality. 
        To mitigate this bias, we use an \shortname{} model, pre-trained on a \textit{random bootstrapping dataset}. The random bootstrapping dataset is generated by picking extractions for each sentence randomly from any one of the bootstrapping systems being aggregated. We assign a score to each extraction in the pool based on the confidence value given to it by this \shortname~(Random) model.

        \vspace{0.5ex}
        \noindent \textbf{Filtering:}
        We now filter this set of extractions for redundancy. 
        Given the set of ranked extractions in the pool, we wish to select that subset of extractions that have the best confidence scores (assigned by the random-boostrap model), while having minimum similarity to the other selected extractions.

        We model this goal as the selection of an optimal subgraph from a suitably designed complete weighted graph. Each node in the graph corresponds to one extraction in the pool. Every pair of nodes $(u,v)$ are connected by an edge. Every edge has an associated weight $R(u,v)$ signifying the similarity between the two corresponding extractions. Each node $u$ is assigned a score $f(u)$ equal to the confidence given by the random-bootstrap model.

        Given this graph $G = (V, E)$ of all pooled extractions of a sentence, we aim at selecting a subgraph $G' = (V', E')$ with $V' \subseteq V$, such that the most significant ones are selected, whereas the extractions redundant with respect to already-selected ones are discarded. Our objective is
        \begin{equation} \label{eq:1}
        \max_{G'\subseteq G} \sum_{i=1}^{|V'|} f(u_i) -
        \sum_{j=1}^{|V'|-1}\sum_{k=j+1}^{|V'|} R(u_{j},u_{k}),
        \end{equation}
        where $u_{i}$ represents node $i\in V'$.  
        %Here, $f(u_{i})$ represents node score for vertex $u_{i}$, i.e., the score for each triple obtained using the random-bootstrap model. 
        We compute $R(u,v)$ as the ROUGE2 score between the serialized triples represented by nodes $u$ and $v$. We can intuitively understand the first term as the aggregated sum of significance of all selected triples and second term as the redundancy among these triples.

        If $G$ has $n$ nodes, we can pose the above objective as:
        \begin{align}
        \max_{\bm{x} \in \{0,1\}^{n}}  \bm{x}^\top \bm{f} - \bm{x}^\top \bm{R} \bm{x},
        \end{align}
        where $\bm{f} \in \mathbb{R}^n$ representing the node scores, i.e., $f[i]=f(u_{i})$, and
        $\bm{R} \in \mathbb{R}^{n\times n}$ is a symmetric matrix with entries $R_{j,k}=\text{ROUGE2}(u_{j},u_{k})$.
        $\bm{x}$ is the decision vector, with $x[i]$ indicating whether a particular node $u_i \in V'$ or not. This is an instance of Quadratic Boolean Programming and is NP-hard, but in our application $n$ is modest enough that this is not a concern.  We use the QPBO (Quadratic Pseudo Boolean Optimizer) solver\footnote{\href{https://pypi.org/project/thinqpbo/}{https://pypi.org/project/thinqpbo/}} \cite{Rother2007OptimizingBM} to find the optimal $\bm{x}^*$ and recover~$V'$.

\section{Experimental Setup}
    \subsection{Training Data Construction}

        %To train a seq2seq model, we require a large training corpus of labeled data. 
        We obtain our training sentences by scraping Wikipedia, because Wikipedia is a comprehensive source of informative text from diverse domains, rich in entities and relations.  
        Using sentences from Wikipedia ensures that our model is not biased towards data from any single domain.

        We run OpenIE-4\footnote{\href{https://github.com/knowitall/openie}{https://github.com/knowitall/openie}}, ClausIE\footnote{\href{https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/software/clausie}{https://www.mpi-inf.mpg.de/clausie}} and RnnOIE\footnote{\href{https://github.com/gabrielStanovsky/supervised-oie}{https://github.com/gabrielStanovsky/supervised-oie}} on these sentences to generate a set of OpenIE tuples for every sentence, which are then ranked and filtered using our Score-and-Filter technique. 
        These tuples are further processed to generate training instances in \shortname's input-output format. 

        Each sentence contributes to multiple (input, output) pairs for the \shortname{} model. The first training instance contains the sentence itself as input and the first tuple as output. For example, (``I ate an apple and an orange.'', ``I; ate; an apple''). The next training instance, contains the sentence concatenated with previous tuple as input and the next tuple as output (``I ate an apple and an orange. [SEP] I; ate; an apple'',  ``I; ate; an orange''). The final training instance generated from this sentence includes all the extractions appended to the sentence as input and \textit{EndOfExtractions} token as the output. Every sentence gives the seq2seq learner one training instance more than the number of tuples. 

        While forming these training instances, the tuples are considered in decreasing order of their confidence scores.  If some OpenIE system does not provide confidence scores for extracted tuples, then the output order of the tuples may be used.  

    \subsection{Dataset and Evaluation Metrics}

        We use the CaRB data and evaluation framework \cite{bhardwaj&al19} to evaluate the systems\footnote{Our reported CaRB scores for OpenIE-4 and OpenIE-5 are slightly different from those reported by \citet{bhardwaj&al19}. The authors of CaRB have verified our values.} at different confidence thresholds, yielding a precision-recall curve. We identify three important summary metrics from the P-R curve. 

        \todo{resolve diff in scores in carb and imojie paper, remove footnote}

        \noindent \textbf{Optimal F1}: We find the point in the P-R curve corresponding to the largest F1 value and report that.  This is the operating point for getting extractions with the best precision-recall trade-off.

        \noindent \textbf{AUC}: This is the area under the P-R curve. This metric is useful when the downstream application can use the confidence value of the extraction.

        \noindent \textbf{Last F1}: This is the F1 score computed at the point of zero confidence. This is of importance when we cannot compute the optimal threshold, due to lack of any gold-extractions for the domain. Many downstream applications of OpenIE, such as text comprehension \citep{stanovsky&al15} and sentence similarity estimation \citep{janara&al14}, use \emph{all} the extractions output by the OpenIE system. Last~F1 is an important measure for such applications.

    \subsection{Comparison Systems}
        % TABLE - SCORES TABLE
        \begin{table}[h]
            \centering {\footnotesize
            \begin{tabular}{lccc}
            \hline
            \textbf{System} & \multicolumn{3}{c}{\textbf{Metric}}  \\
            & \multicolumn{1}{c}{Opt. F1} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Last F1}\\
            \hline
            Stanford-IE & 23 & 13.4 & 22.9 \\
            OllIE & 41.1 & 22.5 & 40.9 \\ 
            PropS & 31.9 & 12.6 & 31.8 \\ 
            MinIE & 41.9 & -$^*$ & 41.9 \\ 
            OpenIE-4 & 51.6 & 29.5 & 51.5 \\ 
            OpenIE-5 & 48.5 & 25.7 & 48.5 \\ 
            ClausIE & 45.1 & 22.4 & 45.1 \\ \hline 
            CopyAttention & 35.4 & 20.4 & 32.8 \\ 
            RNN-OIE & 49.2 & 26.5 & 49.2 \\ 
            Sense-OIE & 17.2 & -$^*$ & 17.2 \\ 
            Span-OIE & 47.9 & -$^*$ & 47.9 \\
            CopyAttention + BERT & 51.6 & 32.8 & 49.6 \\ \hline
            % \shortname~(OpenIE-4) & 53.1 & \textbf{33.5} & 52.6 \\ 
            \shortname~ & \textbf{53.5} & \textbf{33.3} & \textbf{53.3} \\ \hline
            \end{tabular} }
            \caption{Comparison of various OpenIE systems - non-neural, neural and proposed models. (*)~Cannot compute AUC as Sense-OIE, MinIE do not emit confidence values for extractions and released code for Span-OIE does not provision calculation of confidence values. In these cases, we report the Last F1 as the Opt. F1}
            % as system does not give confidence values for extractions.}
            \label{tab:scores-table}
        \end{table}    
    
        We compare \shortname{} against several non-neural baselines, including Stanford-IE, OpenIE-4, OpenIE-5, ClausIE, PropS, MinIE, and OLLIE. We also compare against the sequence labeling baselines of RnnOIE, SenseOIE, and the span selection baseline of SpanOIE. Probably the most closely related baseline to us is the neural generation baseline of CopyAttention. To increase CopyAttention's diversity, we compare against an English version of Logician, which adds coverage attention to a single-decoder model that emits all extractions one after another. We also compare against CopyAttention augmented with diverse beam search \cite{diversebeam} --- it adds a diversity term to the loss function so that new beams have smaller redundancy with respect to all previous beams.

        Finally, because our model is based on BERT, we reimplement CopyAttention with a BERT encoder --- this forms a very strong baseline for our task. Table \ref{tab:scores-table} enlists the CaRB scores of these systems.

    \subsection{Implementation}

        We implement \shortname{} in the AllenNLP framework\footnote{\href{https://github.com/allenai/allennlp}{https://github.com/allenai/allennlp}} \citep{gardner2018allennlp} using Pytorch~1.2.  We use ``BERT-small'' model for faster training. Other hyper-parameters include learning rate for BERT, set to $2\times10^{-5}$, and learning rate, hidden dimension, and word embedding dimension of the decoder LSTM, set to $(10^{-3}, 256, 100)$, respectively.

        Since the model or code of CopyAttention \citep{cui&al18} were not available, we implemented it ourselves. Our implementation closely matches their reported scores, achieving (F1, AUC) of (56.4, 47.7) on the OIE2016 benchmark.

\section{Results and Analysis}

    \subsection{Performance of Existing Systems}

        \emph{How well do the neural systems perform as compared to the rule-based systems?}

        Using CaRB evaluation, we find that, contrary to previous papers, neural OpenIE systems are not necessarily better than prior non-neural systems (Table~\ref{tab:scores-table}).  Among the systems under consideration, the best non-neural system reached Last~F1 of 51.5, whereas the best existing neural model could only reach~49.2. Deeper analysis reveals that CopyAttention produces redundant extractions conveying nearly the same information, which CaRB effectively penalizes.  RnnOIE performs much better, however suffers due to its lack of generating auxilliary verbs and implied prepositions. Example, it can only generate (Trump; President; US) instead of (Trump; is President of; US) from the sentence ``US President Trump...". Moreover, it is trained only on limited number of pseudo-gold extractions, generated by \citet{michael&al18}, which does not take advantage of boostrapping techniques.


        \vspace{0.5ex}
    \subsection{Performance of \shortname}
        \emph{How does \shortname{} perform compared to the previous neural and rule-based systems?} 
        % \noindent{\bfseries Performance of \shortname:}  

        % TABLE - IMOJIE vs OpenIE-4 Example
        \begin{table*}[h]
            \centering
            {\small
            \resizebox{\textwidth}{!}{%
            \begin{tabular}{|c|l|}
            \hline
            \textbf{Sentence} & \begin{tabular}[c]{@{}l@{}}Greek and Roman pagans , who saw their relations with the gods in political and social\\  terms , scorned the man who constantly trembled with fear at the thought of the gods ,\\  as a slave might fear a cruel and capricious master .\end{tabular} \\ \hline
            \textbf{OpenIE-4} & ( the man ;  constantly trembled ;   ) \\ \hline
            \textbf{\begin{tabular}[c]{@{}c@{}}IMOJIE\\ \end{tabular}} & \begin{tabular}[c]{@{}l@{}}( a slave ; might fear ; a cruel and capricious master )\\ ( Greek and Roman pagans ; scorned ; the man who ... capricious master )\\ ( the man ; constantly trembled ; with fear at the thought of the gods )\\ ( Greek and Roman pagans ; saw ; their relations with the gods in political and social terms )\end{tabular} \\ \hline
            \end{tabular}}
            }
            \caption{\shortname~ vs. OpenIE-4. Pipeline nature of OpenIE-4 can get confused by long convoluted sentences, but \shortname{} responds gracefully.}
            \label{tab:quality-example}
        \end{table*}

        In comparison with existing neural and non-neural systems, \shortname{} trained on aggregated bootstrapped data performs the best.  It outperforms OpenIE-4, the best existing OpenIE system, by 1.9 F1 pts, 3.8 pts of AUC, and 1.8 pts of Last-F1. Qualitatively, we find that it makes fewer mistakes than OpenIE-4, probably because OpenIE-4 accumulates errors from upstream parsing modules (see Table \ref{tab:quality-example}). 

        % TABLE - REDUNDANCY EXAMPLE (IMoJIE vs Copy Attention)
        \begin{table*}[h]
            \centering
            {\small
            \resizebox{\textwidth}{!}{%
            \begin{tabular}{|c|l|}
            \hline
            \textbf{Sentence} & \begin{tabular}[c]{@{}l@{}}He was appointed Commander of the Order of the British Empire in the 1948\\  Queen's Birthday Honours and was knighted in the 1953 Coronation Honours .\end{tabular} \\ \hline
            \textbf{CopyAttention} & \begin{tabular}[c]{@{}l@{}}( He ; was appointed ; Commander ... Birthday Honours )\\( He ; was appointed ; Commander ... Birthday Honours and was knighted ... Honours )\\( Queen 's Birthday Honours ; was knighted ; in the 1953 Coronation Honours )\\( He ; was appointed ; Commander of the Order of the British Empire in the 1948 )\\( the 1948 ; was knighted ; in the 1953 Coronation Honours)\end{tabular} \\ \hline
            \textbf{\begin{tabular}[c]{@{}l@{}}IMOJIE\\ \end{tabular}} & \begin{tabular}[c]{@{}l@{}}( He ; was appointed ; Commander of the Order ... Birthday Honours )\\( He ; was knighted ; in the 1953 Coronation Honours )\end{tabular} \\ \hline
            \end{tabular}}
            }
            \caption{\shortname~ vs. CopyAttention. CopyAttention suffers from stuttering, which \shortname{} does not.}
            \label{tab:redundancy-example}
        \end{table*}

        \shortname{} outperforms CopyAttention by large margins -- about 18 Optimal F1 pts and 13 AUC pts. Qualitatively, it outputs non-redundant extractions through the use of its iterative memory (see Table~\ref{tab:redundancy-example}), and a variable number of extractions owing to the \textit{EndofExtractions} token.
        It also outperforms CopyAttention with BERT, which is a very strong baseline, by 1.9 Opt. F1 pts, 0.5 AUC and 3.7 Last F1 pts. \shortname{} consistently outperforms CopyAttention with BERT over different bootstrapping datasets (see Table~\ref{tab:bootstrapping-datasets}).

        \begin{figure}[t]
            \centering
        \vspace*{-3ex}
        \includegraphics[scale=0.80]{images/imojie/imojie.png}
        \caption{Precision-Recall curve of OpenIE Systems.}
        \label{fig:pr-curve}
        \end{figure}

        Figure~\ref{fig:pr-curve} shows that the precision-recall curve of \shortname{} is consistently above that of existing OpenIE systems, emphasizing that \shortname{} is consistently better than them across the different confidence thresholds. We do find that CopyAttention+BERT outputs slightly higher recall at a significant loss of precision (due to its beam search with constant size), which gives it some benefit in the overall AUC. CaRB evaluation of SpanOIE\footnote{\href{https://github.com/zhanjunlang/Span\_OIE}{https://github.com/zhanjunlang/Span\_OIE}} results in (precision, recall, F1) of (58.9, 40.3, 47.9). SpanOIE sources its training data only from OpenIE-4. In order to be fair, we compare it against \shortname{} trained only on data from OpenIE-4 which evaluates to (60.4, 46.3, 52.4). Hence, \shortname{} outperforms SpanOIE, both in precision and recall.

        Attention is typically used to make the model focus on words which are considered important for the task. But the \shortname{} model successfully uses attention to \emph{forget} certain words, those which are already covered. 
        
        Consider, the sentence ``He served as the first prime minister of Australia and became a founding justice of the High Court of Australia''. 
        Given the previous extraction (He; served; as the first prime minister of Australia), the BERTâ€™s attention layers figure out that the words `prime' and `minister' have already been covered, and thus push the decoder to prioritize `founding' and `justice'.
        Appendix~\ref{sec:visualize_attention} analyzes the attention patterns of the model when generating the intermediate extraction in the above example and shows that \shortname{} gives less attention to already covered words.

        \vspace{0.5ex}
    \subsection{Redundancy}
        \emph{What is the extent of redundancy in \shortname{} when compared to earlier OpenIE systems?}

        % TABLE - REDUNDANCY REMOVAL EXPERIMENT
        \begin{table}[h]
            \centering {\footnotesize
            \begin{tabular}{lccc}
            \hline
            \textbf{System} & \multicolumn{3}{c}{\textbf{Metric}}  \\
            & \multicolumn{1}{c}{Opt. F1} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Last F1}\\
            \hline
            CopyAttention & 35.4 & 20.4 & 32.8 \\ 
            CoverageAttention & 41.8 & 22.1 & 41.8 \\ 
            CoverageAttention+BERT & 47.9 & 27.9 & 47.9 \\ 
            Diverse Beam Search & 46.1 & 26.1 & 39.6 \\ 
            \shortname~ (w/o BERT) & 37.9 & 19.1 & 36.6 \\ 
            \shortname~ & \textbf{53.2} & \textbf{33.1} & \textbf{52.4} \\ \hline
            \end{tabular} }
            \caption{Models to solve the redundancy issue prevalent in Generative Neural OpenIE systems. All systems are bootstrapped on OpenIE-4.}
            \label{tab:redundancy_rem_exp}
        \end{table}

        We also investigate other approaches to reduce redundancy in CopyAttention, such as Logician's coverage attention (with both an LSTM and a BERT encoder) as well as diverse beam search. 
        Table~\ref{tab:redundancy_rem_exp} reports that both these approaches indeed make significant improvements on top of CopyAttention scores. 
        In particular, qualitative analysis of diverse beam search output reveals that the model gives out different words in different tuples in an effort to be diverse, without considering their correctness. 
        Moreover, since this model uses beam search, it still outputs a fixed number of tuples.

        % TABLE - BOOTSTRAPPING DATASETS
        \begin{table*}
        \centering {\footnotesize
        \begin{tabular}{lcccc}
        \hline
        \textbf{System} & \multicolumn{4}{c}{\textbf{Bootstrapping System}}  \\
        & \multicolumn{1}{c}{OpenIE-4} & \multicolumn{1}{c}{OpenIE-5} & \multicolumn{1}{c}{ClausIE} & \multicolumn{1}{c}{RnnOIE}\\
        \hline
        Base & 50.7, 29, 50.7 & 47.4, 25.1, 47.4 & 45.1, 22.4, 45.1 & 49.2, 26.5, 49.2 \\ 
        CopyAttention+BERT & 51.6, 32.8, 49.6 & 48.7, \textbf{29.4}, 48.0 & 47.4, 30.2, 43.6 & 47.9, 30.6, 41.1 \\ 
        \shortname & \textbf{53.2}, \textbf{33.1}, \textbf{52.4} & \textbf{48.8}, 27.9, \textbf{48.7} & \textbf{49.2}, \textbf{31.4}, \textbf{45.5} & \textbf{51.3}, \textbf{31.1}, \textbf{50.8} \\ \hline
        \end{tabular} }
        \caption{Evaluating models trained with different bootstrapping systems.}
        \label{tab:bootstrapping-datasets}
        \end{table*}

        This analysis naturally suggested the \shortname~(w/o BERT) model --- an \shortname{} variation that uses an LSTM encoder instead of BERT. Unfortunately, \shortname~(w/o BERT) is behind the CopyAttention baseline by 12.1 pts in AUC and 4.4 pts in Last~F1. We hypothesize that this is because the LSTM encoder is unable to learn how to capture \textit{inter-fact dependencies} adequately --- the input sequences are too long for effectively training LSTMs.

        This explains our use of Transformers (BERT) instead of the LSTM encoder to obtain the final form of \shortname.  With a better encoder, \shortname{} is able to perform up to its potential, giving an improvement of \textbf{(17.8, 12.7, 19.6)} pts in (Optimal~F1, AUC, Last~F1) over existing seq2seq OpenIE systems.

        We further measure two quantifiable metrics of redundancy:
        \begin{description}
        \item[Mean Number of Occurrences (MNO):] The average number of tuples, every output word appears in.
        \item[Intersection Over Union (IOU):] Cardinality of intersection over cardinality of union of words in the two tuples, averaged over all pairs of tuples.
        \end{description}

        These measures were calculated after removing stop words from tuples.  Higher value of these measures suggest higher redundancy among the extractions.  \shortname{} is significantly better than CopyAttention+BERT, the strongest baseline, on both these measures (Table~\ref{tab:RED}). Interestingly, \shortname{} has a lower redundancy than even the gold triples; this is due to imperfect recall.


        % TABLE - REDUNDANCY Experiments
        \begin{table}[h]
            \centering {\footnotesize
            \begin{tabular}{lccc}
            \hline
            \textbf{Extractions} & \multicolumn{3}{c}{\textbf{Metric}}  \\
            & \multicolumn{1}{c}{MNO} & \multicolumn{1}{c}{IOU} & \multicolumn{1}{c}{\#Tuples}\\
            \hline
            CopyAttention+BERT &  2.805 & 0.463 & 3159 \\ 
            \textbf{IMOJIE} & \textbf{1.282} & \textbf{0.208} & \textbf{1620} \\ 
            Gold & 1.927 & 0.31 & 2650 \\ \hline
            \end{tabular} }
            \caption{Measuring redundancy of extractions. MNO stands for Mean Number of Occurrences. IOU stands for Intersection over Union.}
            \label{tab:RED}
        \end{table}

        \vspace{0.5ex}
    \subsection{The Value of Iterative Memory}

        \emph{To what extent does the \shortname~style of generating tuples improve performance, over and above the use of BERT?} 

        We add BERT to CopyAttention model to generate another baseline for a fair comparison against the \shortname{} model. When trained only on OpenIE-4, \shortname{} continues to outperform CopyAttention+BERT baseline by (1.6, 0.3, 2.8) pts in (Optimal~F1, AUC, Last~F1), which provides strong evidence that the improvements are not solely by virtue of using a better encoder.
        We repeat this experiment over different (single) bootstrapping datasets. Table \ref{tab:bootstrapping-datasets} depicts that \shortname~consistently outperforms CopyAttention+BERT model.

        We also note that the order in which the extractions are presented to the model (during training) is indeed important. On training IMoJIE using a randomized-order of extractions, we find a decrease of 1.6 pts in AUC (averaged over 3 runs).

        % \vspace{0.5ex}
        % \noindent\textbf{Ablation Study over Bootstrapping Datasets:} We now assess the value of bootstrapping from multiple extraction systems. The ablation study over the datasets (Table \ref{tab:dataset-ablation}) suggests that the model trained on all three aggregated datasets perform better than models trained on either of the single/doubly-aggregated datasets.
        \vspace{0.5ex}
    \subsection{The value of Score-and-Filter}
        % \noindent\textbf{The Value of Score-and-Filter:} 
        \emph{To what extent does the scoring and filtering approach lead to improvement in performance?} 

        \shortname{} aggregates extractions from multiple systems through the scoring and filtering approach. It uses extractions from OpenIE-4 (190K), ClausIE (202K) and RnnOIE (230K) to generate a set of 215K tuples. Table \ref{tab:dataset-aggregation1} reports that \shortname{} does not perform well when this aggregation mechanism is turned off. We also try two supervised approaches to aggregation, by utilizing the gold extractions from CaRB's dev set.
        
        \begin{table}[h]
            \centering {\footnotesize
            \begin{tabular}{lccc}
            \hline
            \textbf{Filtering} & \multicolumn{3}{c}{\textbf{Metric}}  \\
            & \multicolumn{1}{c}{Opt. F1} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Last F1}\\
            \hline
            None & 49.7 & \textbf{34.5} & 37.4 \\ 
            Extraction-based & 46 & 29.2 & 44.9 \\ 
            Sentence-based &  49.5 & 32.7 & 48.6\\ 
            Score-And-Filter & \textbf{53.5} & 33.3 & \textbf{53.3} \\ \hline
            \end{tabular} }
            \caption{Performance of \shortname{} on aggregated dataset \textbf{OpenIE-4+ClausIE+RnnOIE}, with different filtering techniques. For comparison, SenseOIE trained on multiple system extractions gives an F1 of 17.2 on CaRB.}
            \label{tab:dataset-aggregation1}
        \end{table}

        \begin{itemize}
            \item \textbf{Extraction Filtering}: For every sentence-tuple pair, we use a binary classifier that decides whether or not to consider that extraction. The input features of the classifier are the \textit{[CLS]}-embeddings generated from BERT after processing the concatenated sentence and extraction. The classifier is trained over tuples from CaRB's dev set.
            \item \textbf{Sentence Filtering}: We use an \shortname{} model (bootstrapped over OpenIE-4), to score all the tuples. Then, a Multilayer Perceptron (MLP) predicts a confidence threshold to perform the filtering. Only extractions with scores greater than this threshold will be considered. The input features of the MLP include the length of sentence, \shortname~ (OpenIE-4) scores, and GPT \cite{Radford2018-GPT} scores of each extraction. This MLP is trained over sentences from CaRB's dev set and the gold optimal confidence threshold calculated by CaRB. 
        \end{itemize}
        We observe that the Extraction, Sentence Filtering are better than no filtering by by 7.5, 11.2 pts in Last F1, but worse at Opt. F1 and AUC. We hypothesise that this is because the training data for the MLP (640 sentences in CaRB's dev set), is not sufficient and the features given to it are not sufficiently discriminative.
        Thereby, we see the value of our unsupervised Score-and-Filter that improves the performance of \shortname{} by (3.8, 15.9) pts in (Optimal F1, Last F1). The 1.2 pt decrease in AUC is due to the fact that the \shortname{} (no filtering) produces many low-precision extractions, that inflates the AUC.


        % TABLE - DATASET ABLATION
        \begin{table}[h]
            \centering {\footnotesize
            \begin{tabular}{lccc}
            \hline
            \textbf{Bootstrapping } & \multicolumn{3}{c}{\textbf{Metric}}  \\
            \textbf{Systems} & \multicolumn{1}{c}{Opt. F1} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Last F1} \\
            \hline
            ClausIE & 49.2 & 31.4 & 45.5 \\
            RnnOIE & 51.3 & 31.1 & 50.8 \\
            OpenIE-4 & 53.2 & 33.1 & 52.4 \\
            OpenIE-4+ClausIE & 51.5 & 32.5 & 47.1 \\ 
            OpenIE-4+RnnOIE & 53.1 & 32.1 & 53.0 \\ 
            ClausIE+RnnOIE & 50.9 & 32.2 & 49.8 \\ 
            All & \textbf{53.5} & \textbf{33.3} & \textbf{53.3} \\ \hline
            \end{tabular} }
            \caption{\shortname{} trained with different combinations of bootstrapping data from 3 systems - OpenIE-4, ClausIE, RNNOIE. Graph filtering is not used over single datasets.}
            \label{tab:dataset-ablation}
        \end{table}

        Table \ref{tab:dataset-ablation} suggests that the model trained on all three aggregated datasets perform better than models trained on any of the single/doubly-aggregated datasets. Directly applying the Score-and-Filter method on the test-extractions of RnnOIE+OpenIE-4+ClausIE gives (Optimal F1, AUC, Last F1) of (50.1, 32.4, 49.8). This shows that training the model on the aggregated dataset is important.

        \vspace{0.5ex}
        \noindent\textbf{Computational Cost}:
        The training times for CopyAttention+BERT, \shortname~ (OpenIE-4) and \shortname~ (including the time taken for Score-and-Filter) are 5 hrs, 13 hrs and 30 hrs respectively. This shows that the performance improvements come with an increased computational cost, and we leave it to future work to improve the computational efficiency of these models.


\section{Error Analysis}
    We randomly selected 50 sentences from the CaRB validation set. We consider only sentences where at least one of its extractions shows the error. We identified four major phenomena contributing to errors in the \shortname{} model: \newline
    (1) \textbf{Missing information:} 66\% of the sentences have at least one of the relations or arguments or both missing in predicted extractions, which are present in gold extractions. This leads to incomplete information. \newline
    (2) \textbf{Incorrect demarcation:} Extractions in 60\% of the sentences have the separator between relation and argument identified at the wrong place. \newline
    %For e.g., in sentence ``In October 2008 , Bond apologized to former U.S. Attorney ....", our system predicts (Bond; apologized; to former U.S. Attorney ....) whereas it should have been (Bond; apologized to; former U.S. Attorney ....). 
    (3) \textbf{Missing conjunction splitting:} In 32\% of the sentences, our system fails to separate out extractions by splitting a conjunction.  E.g., in the sentence ``US 258 and NC 122 parallel the river north \dots'', \shortname{} predicts just one extraction (US 258 and NC 122; parallel; \dots) as opposed to two separate extractions (US 258; parallel; \dots) and (NC 122; parallel; \dots) as in gold. \newline
    (4) \textbf{Grammatically incorrect extractions:} 38\% sentences have a grammatically incorrect extraction (when serialized into a sentence).  \newline
    Additionally, we observe 12\% sentences still suffering from \textbf{redundant} extractions and 4\% \textbf{miscellaneous} errors.

\section{Discussion}

    We propose \shortname~for the task of OpenIE. \shortname~significantly improves upon the existing OpenIE systems in all three metrics, Optimal F1, AUC, and Last F1, establishing a new State Of the Art system. Unlike existing neural OpenIE systems, \shortname~produces non-redundant as well as a variable number of OpenIE tuples depending on the sentence, by iteratively generating them conditioned on the previous tuples. Additionally, we also contribute a novel technique to combine multiple OpenIE datasets to create a high-quality dataset in a completely unsupervised manner. We release the training data, code, and the pretrained models.\footnote{\href{https://github.com/dair-iitd/imojie}{https://github.com/dair-iitd/imojie}}

    \shortname~ presents a novel way of using attention for text generation. \citet{bahdanau&al15} showed that attending over the input words is important for text generation. \citet{see&al17} showed that using a coverage loss to track the attention over the decoded words improves the quality of the generated output. We add to this narrative by showing that deep inter-attention between the input and the partially-decoded words (achieved by adding previous output in the input) creates a better representation for iterative generation of triples. This general observation may be of independent interest beyond OpenIE, such as in text summarization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Uncategorised
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
