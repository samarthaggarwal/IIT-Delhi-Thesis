\section{Overview}
  \subsection{What is OpenIE}

  Extracting structured information from unstructured text has been a key research area within NLP. The paradigm of Open Information Extraction (OpenIE) \cite{banko&al07} uses an open vocabulary to convert natural text to semi-structured representations, by extracting a set of (subject, relation, object) tuples. OpenIE has found wide use in many downstream NLP tasks \cite{mausam16} like multi-document question answering and summarization \cite{fan&al19}, event schema induction \cite{balasubramanian-emnlp13} and word embedding generation \cite{stanovsky&al15}. Table \ref{tab:openie_eg} enlists a few examples of sentences along with their OpenIE tuples.
  
  % table with openie examples
\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{|l|l|}
  \hline
  \multicolumn{1}{|c|}{\textbf{Sentence}} &
    \multicolumn{1}{c|}{\textbf{Open IE Tuples}} \\ \hline
  \begin{tabular}[c]{@{}l@{}}The US President \\ Donald Trump gave \\ his speech on Tuesday \\ to thousands of people.\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}( Donald Trump ; is the president of ; US )\\ ( Donald Trump ; gave ; his speech )\\ ( Donald Trump ; gave his speech on ; Tuesday)\\ ( Donald Trump ; gave his speech to ; thousands of people )\end{tabular} \\ \hline
  \begin{tabular}[c]{@{}l@{}}John likes to play the \\ piano.\end{tabular} &
    ( John ; likes to play ; the piano ) \\ \hline
  \begin{tabular}[c]{@{}l@{}}Solo Piano I is a great \\ album of classical piano \\ compositions.\end{tabular} &
    ( Solo Piano I ; is a great album of ; classical piano compositions ) \\ \hline
  \begin{tabular}[c]{@{}l@{}}John said, "Monday is \\ the first day of the week."\end{tabular} &
    \begin{tabular}[c]{@{}l@{}}( John ; said ; "Monday is the first day of the week" )\\ ( {[}Context : John said{]} Monday ; is ; the first day of the week )\end{tabular} \\ \hline
  \end{tabular}%
  }
  \caption{Example of Open IE tuples of some sample sentences}
  \label{tab:openie_eg}
  \end{table}

  The task of Open Information Extraction (OpenIE) involved listing out all possible inferences from a given sentence. Each OpenIE extraction typically comprises of a relation and two arguments. However, this structure is not strictly enforced and one or more of these components may be absent from a valid OpenIE tuple. Multiple formats have been proposed for output of OpenIE:
  \begin{enumerate}
    \item N-ary Format : In this format, all the arguments corresponding to the same relation and (subject)argument are stacked within a single extraction, albeit the argument boundary is maintained. For eg.
\begin{verbatim}
Sentence:
``The US president Donald Trump gave his speech on Tuesday to thousands
of people.''
Extractions:
(Donald Trump ; gave ; his speech ; on Tuesday ; to thousands of people)
\end{verbatim}

    \item Binary Format : In this format, all the arguments corresponding to the same relation and (subject)argument are assigned to separate tuples. For eg.
\begin{verbatim}
  Sentence:
  ``The US president Donald Trump gave his speech on Tuesday to thousands
  of people.''
  Extractions:
  (Donald Trump ; gave ; his speech)
  (Donald Trump ; gave his speech on ; Tuesday)
  (Donald Trump ; gave ; his speech to ; thousands of people)
\end{verbatim}
  \end{enumerate}

  Notice that the N-ary format keeps the prepositions preceeding along with the respective arguments whereas the binary format moves them along with the relation. Although, the inter-conversion between the two formats is easy, we will focus on the binary format for the rest of the thesis due to its relatively higher popularity among the recent systems.
    
  \subsection{Performance Gaps}

  There have been many Open IE systems till date such as TextRunner \citep{TextRunner}, ReVerb \citep{ReVerb1, ReVerb2}, OLLIE \citep{Ollie}, ClausIE \citep{Clausie}, OpenIE 4 \citep{Openie-4a, Openie-4b}, OpenIE 5 \citep{OpenIE-5a, OpenIE-5b}, PropS \citep{PropS}, NST \citep{Nst},  Neural Open IE \citep{cui&al18}, and more. With the advent of so many systems, it is imperative to have a standardized mechanism for automatic evaluation so that they can be compared. This led to Open IE benchmarking systems such as RelVis \citep{Relvis}, Wire57 \citep{Wire57} and OIE2016 \citep{OIE2016}.

  However, OpenIE systems still perform unsatisfactorily when compared against the gold OpenIE outputs. The traditional rule-based OpenIE systems have a large number of components that cause errors to be cascaded. On the other hand, the neural systems introduce other problem such as high degree of redundancy among extractions, fixed number of extractions due to a beam search, and more. These problems indicate a need for an improved OpenIE system that can cater most, if not all, of these problems.
  
  Upon manually examining the OpenIE systems and their evaluation, we figured out an even deeper rooted problem. The benchmarks used to evaluate these systems were themselves imperfect. They suffer from two major flaws: small and noisy gold datasets, and non-standard evaluation policies. Wire57 \citep{Wire57} uses a tiny gold dataset of only 57 sentences. RelVis had non-intuitive evaluation policies. OIE2016, which was the state-of-the-art OpenIE benchmark at that time, also had a noisy gold dataset (refer to table \ref{tab:gold_example}) as well as non-intuitive schemes of evaluation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statement}

  It was clear that creation of a reliable OpenIE benchmark had to preceed the creation of a better OpenIE system. Hence, we were faced with the following two problem statements:

  \textit{Problem 1:} Establish a new state-of-the-art in OpenIE benchmarking, with a large high-quality gold dataset and evaluation policies that correlate well with human judgement.

  \textit{Problem 2:} Develop a new state-of-the-art OpenIE system that could overcome that the problems of existing ones.
  
% \section{Motivation}
  The motivation to improve OpenIE came from the benefits that would percolate to its downstream applications. Open IE has numerous downstream applications such as knowledge base construction, relation extraction, summarisation and learning word embeddings \citep{embedding,survey}. Improving the performance of OpenIE systems would directly translate to an enhancement in their performance as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contributions}

  Our major contributions are:
  \begin{itemize}
    \item We contribute CaRB, an improved dataset and framework for testing Open IE systems. To the best of our knowledge, CaRB is the first \emph{crowdsourced} Open IE dataset and it also makes substantive changes in the matching code and metrics. NLP experts annotate CaRB's dataset to be more accurate than OIE2016. Moreover, we find that on one  pair of Open IE systems, CaRB framework provides contradictory results to OIE2016. Human assessment verifies that CaRB's ranking of the two systems is the accurate ranking. We release the CaRB framework along with its crowdsourced dataset.

    \item We contribute \shortname{}, a neural OpenIE system that generates the next extraction, fully conditioned on the extractions produced so far. \shortname{} produce a variable number of diverse extractions for a sentence,
    \item We present an unsupervised aggregation scheme to bootstrap training data by combining extractions from multiple OpenIE systems.
    
    \item We present a new state-of-the-art Coordination Analyzer which is 13 pts better in F1 over previous best reported results
    \item We contribute a Multi Level Iterative Labelling (MLIL) architecture based OpenIE system that is trained using constraints to improve recall. MLIL based OpenIE estalishes itself as a new state-of-the-art in OpenIE systems. It is 25x faster then \shortname\ and improves the F1 score by as much as 3.8 pts.
  \end{itemize}

